{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T04:28:15.253696Z",
     "start_time": "2024-11-24T04:28:13.709524Z"
    }
   },
   "cell_type": "code",
   "source": "pip install selenium pandas",
   "id": "d898a7f1a9754d9d",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:47:41.840164Z",
     "start_time": "2024-11-24T20:37:52.398770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/?BD=7200&TP=107\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 3: Wait for the first results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 4: Scroll and Load More Results\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "    while True:\n",
    "        # Find all articles\n",
    "        new_articles = driver.find_elements(By.CSS_SELECTOR, \"article.post.yes\")\n",
    "        articles.extend(new_articles)\n",
    "\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow time for new content to load\n",
    "\n",
    "        # Check if the page height has changed\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 5: Extract Information from Each Article\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        title = article.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "        license_number = article.find_element(By.CSS_SELECTOR, \"span[id^='lic']\").text\n",
    "        license_type = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Type:')]]\").text\n",
    "        license_status = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Status:')]]\").text\n",
    "        expiration_date = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Expiration Date:')]]\").text\n",
    "        city = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'City:')]]\").text\n",
    "        state = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'State:')]]\").text\n",
    "        county = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'County:')]]\").text\n",
    "        zip_code = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Zip:')]]\").text\n",
    "\n",
    "        results.append({\n",
    "            \"Title\": title,\n",
    "            \"License Number\": license_number,\n",
    "            \"License Type\": license_type,\n",
    "            \"License Status\": license_status,\n",
    "            \"Expiration Date\": expiration_date,\n",
    "            \"City\": city,\n",
    "            \"State\": state,\n",
    "            \"County\": county,\n",
    "            \"Zip\": zip_code,\n",
    "        })\n",
    "\n",
    "    # Step 6: Save to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacy_data.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "fb0fee304781794b",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:48:07.842137Z",
     "start_time": "2024-11-24T20:48:07.801942Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "8e134c9e3658545",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:34:26.102707Z",
     "start_time": "2024-11-24T21:24:47.375151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Second Attempt; Issue with the above is that it is not searching for specifically sterile compound pharmacies so we are getting the wrong info back and i have the wrong table format.\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/?BD=7200&TP=107\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the License Type dropdown to be present\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"licenseType\"))\n",
    "    )\n",
    "    \n",
    "    # Step 3: Select \"Sterile Compounding Pharmacy\" from the dropdown\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "    \n",
    "    # Step 4: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 5: Wait for the filtered results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 6: Scroll and Load More Results\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "    while True:\n",
    "        # Find all articles\n",
    "        new_articles = driver.find_elements(By.CSS_SELECTOR, \"article.post.yes\")\n",
    "        articles.extend(new_articles)\n",
    "\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow time for new content to load\n",
    "\n",
    "        # Check if the page height has changed\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 7: Extract Information from Each Article\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        title = article.find_element(By.XPATH, \".//li/h3\").text\n",
    "        license_number = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Number:')]]/a/span\").text\n",
    "        license_type = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Type:')]]\").text.split(\": \")[-1]\n",
    "        license_status = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Status:')]]\").text.split(\": \")[-1]\n",
    "        expiration_date = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Expiration Date:')]]\").text.split(\": \")[-1]\n",
    "        city = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'City:')]]/span\").text\n",
    "        state = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'State:')]]/span\").text\n",
    "        county = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'County:')]]\").text.split(\": \")[-1]\n",
    "        zip_code = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Zip:')]]\").text.split(\": \")[-1]\n",
    "\n",
    "        results.append({\n",
    "            \"Title\": title,\n",
    "            \"License Number\": license_number,\n",
    "            \"License Type\": license_type,\n",
    "            \"License Status\": license_status,\n",
    "            \"Expiration Date\": expiration_date,\n",
    "            \"City\": city,\n",
    "            \"State\": state,\n",
    "            \"County\": county,\n",
    "            \"Zip\": zip_code,\n",
    "        })\n",
    "\n",
    "    # Step 8: Save to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"sterile_compounding_pharmacy_data.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "d6c38dbd2dbb4b2a",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:57:08.562472Z",
     "start_time": "2024-11-24T21:48:44.519622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#This is a third attempt; I am now going to use the advanced search and try and limit to just active pharmacies to stop returning so many that are old and cancelled. \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the new webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the License Type dropdown to be present\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Step 4: Select \"Sterile Compounding Pharmacy\" from the License Type dropdown\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 5: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 6: Wait for the filtered results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 7: Scroll and Load More Results\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "    while True:\n",
    "        # Find all articles\n",
    "        new_articles = driver.find_elements(By.CSS_SELECTOR, \"article.post.yes\")\n",
    "        articles.extend(new_articles)\n",
    "\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow time for new content to load\n",
    "\n",
    "        # Check if the page height has changed\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 8: Extract Information from Each Article\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        title = article.find_element(By.XPATH, \".//li/h3\").text\n",
    "        license_number = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Number:')]]/a/span\").text\n",
    "        license_type = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Type:')]]\").text.split(\": \")[-1]\n",
    "        license_status = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Status:')]]\").text.split(\": \")[-1]\n",
    "        expiration_date = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Expiration Date:')]]\").text.split(\": \")[-1]\n",
    "        city = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'City:')]]/span\").text\n",
    "        state = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'State:')]]/span\").text\n",
    "        county = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'County:')]]\").text.split(\": \")[-1]\n",
    "        zip_code = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Zip:')]]\").text.split(\": \")[-1]\n",
    "\n",
    "        results.append({\n",
    "            \"Title\": title,\n",
    "            \"License Number\": license_number,\n",
    "            \"License Type\": license_type,\n",
    "            \"License Status\": license_status,\n",
    "            \"Expiration Date\": expiration_date,\n",
    "            \"City\": city,\n",
    "            \"State\": state,\n",
    "            \"County\": county,\n",
    "            \"Zip\": zip_code,\n",
    "        })\n",
    "\n",
    "    # Step 9: Save to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"sterile_compounding_pharmacy_data_2.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "b0bfe17898dee16c",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:32:24.744101Z",
     "start_time": "2024-11-24T22:29:28.876833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the new webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the License Type dropdown to be present\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Step 4: Select \"Sterile Compounding Pharmacy\" from the License Type dropdown\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 5: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 6: Wait for the filtered results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 7: Scroll and Load More Results\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "    while True:\n",
    "        # Find all articles with a class 'post' and a specific id\n",
    "        article_elements = driver.find_elements(By.CSS_SELECTOR, \"article.post\")\n",
    "\n",
    "        # Loop through each article and extract the data\n",
    "        for article in article_elements:\n",
    "            article_id = article.get_attribute(\"id\")  # Extract article id\n",
    "\n",
    "            title = article.find_element(By.XPATH, \".//li/h3\").text\n",
    "\n",
    "            # Wait until the License Number element is found (with the dynamic 'lic' id)\n",
    "            try:\n",
    "                license_number = WebDriverWait(article, 10).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (By.XPATH, \".//li[strong[contains(text(), 'License Number:')]]/a/span[starts-with(@id, 'lic')]\")\n",
    "                    )\n",
    "                ).text\n",
    "            except:\n",
    "                license_number = \"Not Available\"\n",
    "\n",
    "            license_type = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Type:')]]\").text.split(\": \")[-1]\n",
    "            license_status = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Status:')]]\").text.split(\": \")[-1]\n",
    "            expiration_date = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Expiration Date:')]]\").text.split(\": \")[-1]\n",
    "            city = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'City:')]]/span\").text\n",
    "            state = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'State:')]]/span\").text\n",
    "            county = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'County:')]]\").text.split(\": \")[-1]\n",
    "            zip_code = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Zip:')]]\").text.split(\": \")[-1]\n",
    "\n",
    "            # Add the data to the results list\n",
    "            articles.append({\n",
    "                \"Article ID\": article_id,\n",
    "                \"Title\": title,\n",
    "                \"License Number\": license_number,\n",
    "                \"License Type\": license_type,\n",
    "                \"License Status\": license_status,\n",
    "                \"Expiration Date\": expiration_date,\n",
    "                \"City\": city,\n",
    "                \"State\": state,\n",
    "                \"County\": county,\n",
    "                \"Zip\": zip_code,\n",
    "            })\n",
    "\n",
    "        # Scroll down to load more results\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow time for new content to load\n",
    "\n",
    "        # Check if the page height has changed (to detect if more articles are available)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 8: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"sterile_compounding_pharmacy_data.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "a935c4e67e4a93d1",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Next Attempt ",
   "id": "203cd964fc2f450f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T02:09:16.973969Z",
     "start_time": "2024-11-25T01:25:15.989664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the new webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the License Type dropdown to be present\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Step 4: Select \"Sterile Compounding Pharmacy\" from the License Type dropdown\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 5: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 6: Wait for the filtered results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 7: Scroll and Load More Results\n",
    "    articles = []\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # Find all articles with a numeric id\n",
    "        article_elements = driver.find_elements(By.CSS_SELECTOR, \"article.post\")\n",
    "        \n",
    "        for article in article_elements:\n",
    "            # Extract the numeric ID from the article's `id` attribute\n",
    "            article_id = article.get_attribute(\"id\")\n",
    "            if not re.match(r'^\\d+$', article_id):\n",
    "                continue  # Skip non-numeric IDs\n",
    "            \n",
    "            try:\n",
    "                # Extract details from the <li> elements within the article\n",
    "                li_elements = article.find_elements(By.XPATH, \".//li\")\n",
    "                article_data = {\"Article ID\": article_id}\n",
    "    \n",
    "                for li in li_elements:\n",
    "                    text = li.text.strip()\n",
    "                    \n",
    "                    if \"License Number:\" in text:\n",
    "                        # Extract License Number\n",
    "                        article_data[\"License Number\"] = li.find_element(By.XPATH, \".//a/span[starts-with(@id, 'lic')]\").text\n",
    "                    \n",
    "                    elif \"License Type:\" in text:\n",
    "                        # Extract License Type\n",
    "                        article_data[\"License Type\"] = text.split(\": \")[-1]\n",
    "                    \n",
    "                    elif \"License Status:\" in text:\n",
    "                        # Extract License Status\n",
    "                        article_data[\"License Status\"] = text.split(\": \")[-1]\n",
    "                    \n",
    "                    elif \"Expiration Date:\" in text:\n",
    "                        # Extract Expiration Date\n",
    "                        article_data[\"Expiration Date\"] = text.split(\": \")[-1]\n",
    "                    \n",
    "                    elif \"City:\" in text:\n",
    "                        # Extract City\n",
    "                        article_data[\"City\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "                    \n",
    "                    elif \"State:\" in text:\n",
    "                        # Extract State\n",
    "                        article_data[\"State\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "                    \n",
    "                    elif \"County:\" in text:\n",
    "                        # Extract County\n",
    "                        article_data[\"County\"] = text.split(\": \")[-1]\n",
    "                    \n",
    "                    elif \"Zip:\" in text:\n",
    "                        # Extract Zip Code\n",
    "                        article_data[\"Zip\"] = text.split(\": \")[-1]\n",
    "                \n",
    "                # Add the collected data to the articles list\n",
    "                if article_data not in articles:  # Avoid duplicates\n",
    "                    articles.append(article_data)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article ID {article_id}: {e}\")\n",
    "        \n",
    "        # Scroll down to load more results\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow time for new content to load\n",
    "    \n",
    "        # Check if the page height has changed (to detect if more articles are available)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    # Step 8: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"sterile_compounding_pharmacy_data.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "9bdd3f72853c14bf",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T02:10:05.621276Z",
     "start_time": "2024-11-25T02:10:05.573703Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "212978d8660ec717",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## New Strategy:  Use UL when Class = \"Actions\" to locate LIs ",
   "id": "81d278db270ef2da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T02:17:42.740480Z",
     "start_time": "2024-11-25T02:10:20.386021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the License Type dropdown to be present\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Step 4: Select \"Sterile Compounding Pharmacy\" from the License Type dropdown\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 5: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 6: Wait for the initial results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"actions\"))\n",
    "    )\n",
    "\n",
    "    # Step 7: Scroll and load more results\n",
    "    articles = []\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_pause_time = 2  # Adjust based on network speed\n",
    "\n",
    "    while True:\n",
    "        # Find all <ul> elements with class \"actions\"\n",
    "        ul_elements = driver.find_elements(By.CSS_SELECTOR, \"ul.actions\")\n",
    "        \n",
    "        for ul in ul_elements:\n",
    "            try:\n",
    "                # Initialize a dictionary to store data\n",
    "                article_data = {}\n",
    "\n",
    "                # Extract the pharmacy name (h3 within the <ul>)\n",
    "                try:\n",
    "                    article_data[\"Pharmacy Name\"] = ul.find_element(By.XPATH, \"./li/h3\").text\n",
    "                except:\n",
    "                    article_data[\"Pharmacy Name\"] = \"Not Available\"\n",
    "\n",
    "                # Extract data for each relevant field\n",
    "                for li in ul.find_elements(By.XPATH, \"./li\"):\n",
    "                    text = li.text.strip()\n",
    "\n",
    "                    if \"License Number:\" in text:\n",
    "                        article_data[\"License Number\"] = li.find_element(By.XPATH, \".//a/span\").text\n",
    "\n",
    "                    elif \"License Type:\" in text:\n",
    "                        article_data[\"License Type\"] = text.split(\": \")[-1]\n",
    "\n",
    "                    elif \"License Status:\" in text:\n",
    "                        article_data[\"License Status\"] = text.split(\": \")[-1]\n",
    "\n",
    "                    elif \"Expiration Date:\" in text:\n",
    "                        article_data[\"Expiration Date\"] = text.split(\": \")[-1]\n",
    "\n",
    "                    elif \"Secondary Status:\" in text:\n",
    "                        article_data[\"Secondary Status\"] = text.split(\": \")[-1]\n",
    "\n",
    "                    elif \"City:\" in text:\n",
    "                        article_data[\"City\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "\n",
    "                    elif \"State:\" in text:\n",
    "                        article_data[\"State\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "\n",
    "                    elif \"County:\" in text:\n",
    "                        article_data[\"County\"] = text.split(\": \")[-1]\n",
    "\n",
    "                    elif \"Zip:\" in text:\n",
    "                        article_data[\"Zip\"] = text.split(\": \")[-1]\n",
    "\n",
    "                # Avoid duplicates in the results\n",
    "                if article_data not in articles:\n",
    "                    articles.append(article_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing <ul> element: {e}\")\n",
    "\n",
    "        # Scroll down to load more results\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        # Check if new content has loaded by comparing page height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:  # Break if no new content is loaded\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 8: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"active_sterile_pharmacy_licenses_california.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "8856f6c952d29225",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "1b762a9fbe01b092",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## New Attempt",
   "id": "36177251c5587e67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T02:33:15.961263Z",
     "start_time": "2024-11-25T02:30:56.712825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the License Type dropdown to be present\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Step 4: Select \"Sterile Compounding Pharmacy\" from the License Type dropdown\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 5: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 6: Wait for the initial results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"actions\"))\n",
    "    )\n",
    "\n",
    "    # Step 7: Loop through IDs and scrape data\n",
    "    articles = []\n",
    "\n",
    "    for article_id in range(845):  # IDs from 0 to 844\n",
    "        try:\n",
    "            # Construct the article selector dynamically using XPath\n",
    "            article_selector = f\"//article[@id='{article_id}']\"\n",
    "            \n",
    "            # Locate the article element by XPath\n",
    "            article_element = driver.find_element(By.XPATH, article_selector)\n",
    "            \n",
    "            # Locate the <ul> element with class \"actions\" within the article\n",
    "            ul_element = article_element.find_element(By.CSS_SELECTOR, \"ul.actions\")\n",
    "            \n",
    "            # Initialize a dictionary to store data\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract the pharmacy name (h3 within the <ul>)\n",
    "            try:\n",
    "                article_data[\"Pharmacy Name\"] = ul_element.find_element(By.XPATH, \"./li/h3\").text\n",
    "            except:\n",
    "                article_data[\"Pharmacy Name\"] = \"Not Available\"\n",
    "\n",
    "            # Extract data for each relevant field\n",
    "            for li in ul_element.find_elements(By.XPATH, \"./li\"):\n",
    "                text = li.text.strip()\n",
    "\n",
    "                if \"License Number:\" in text:\n",
    "                    article_data[\"License Number\"] = li.find_element(By.XPATH, \".//a/span\").text\n",
    "\n",
    "                elif \"License Type:\" in text:\n",
    "                    article_data[\"License Type\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"License Status:\" in text:\n",
    "                    article_data[\"License Status\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"Expiration Date:\" in text:\n",
    "                    article_data[\"Expiration Date\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"Secondary Status:\" in text:\n",
    "                    article_data[\"Secondary Status\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"City:\" in text:\n",
    "                    article_data[\"City\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "\n",
    "                elif \"State:\" in text:\n",
    "                    article_data[\"State\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "\n",
    "                elif \"County:\" in text:\n",
    "                    article_data[\"County\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"Zip:\" in text:\n",
    "                    article_data[\"Zip\"] = text.split(\": \")[-1]\n",
    "\n",
    "            # Avoid duplicates in the results\n",
    "            if article_data not in articles:\n",
    "                articles.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article with ID {article_id}: {e}\")\n",
    "\n",
    "    # At the end of the loop, `articles` will contain all the scraped data.\n",
    "    print(f\"Scraped {len(articles)} articles.\")\n",
    "\n",
    "    # Step 8: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt5.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "ce56f55c78adf514",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5256f0d38ed12010"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attempt 6",
   "id": "e597e78d117d33e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T02:46:12.399224Z",
     "start_time": "2024-11-25T02:38:11.692480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters and set them\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Wait for the first results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 5: Scroll and Load More Results\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "    while True:\n",
    "        # Find all articles\n",
    "        new_articles = driver.find_elements(By.CSS_SELECTOR, \"article.post.yes\")\n",
    "        articles.extend(new_articles)\n",
    "\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow time for new content to load\n",
    "\n",
    "        # Check if the page height has changed\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 6: Extract Information from Each Article\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title = article.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "            license_number = article.find_element(By.CSS_SELECTOR, \"span[id^='lic']\").text\n",
    "            license_type = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Type:')]]\").text\n",
    "            license_status = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'License Status:')]]\").text\n",
    "            expiration_date = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Expiration Date:')]]\").text\n",
    "            city = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'City:')]]\").text\n",
    "            state = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'State:')]]\").text\n",
    "            county = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'County:')]]\").text\n",
    "            zip_code = article.find_element(By.XPATH, \".//li[strong[contains(text(), 'Zip:')]]\").text\n",
    "\n",
    "            results.append({\n",
    "                \"Title\": title,\n",
    "                \"License Number\": license_number,\n",
    "                \"License Type\": license_type,\n",
    "                \"License Status\": license_status,\n",
    "                \"Expiration Date\": expiration_date,\n",
    "                \"City\": city,\n",
    "                \"State\": state,\n",
    "                \"County\": county,\n",
    "                \"Zip\": zip_code,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "\n",
    "    # Step 7: Save to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt6.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "1047e992c2a69605",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "ca8a6c4d383e011f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Attempt 7:\n",
    "Last attempt rendered the correct fields but gave duplicates for the first 20 values available, over and over, and ocne removed, we only had 0 through 19 accounted for.  "
   ],
   "id": "f327bf24ea8f0c99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T02:58:15.212161Z",
     "start_time": "2024-11-25T02:55:55.460582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Wait for the first results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 5: Scroll and Load More Results\n",
    "    # We'll iterate through article IDs instead of scrolling indefinitely to avoid repetition.\n",
    "    articles = []\n",
    "    for article_id in range(845):  # Assuming article IDs go from 0 to 844\n",
    "        try:\n",
    "            # Construct the XPath for each article dynamically using the article_id\n",
    "            article_selector = f\"//article[@id='{article_id}']\"\n",
    "            \n",
    "            # Locate the article element using the dynamically generated XPath\n",
    "            article_element = driver.find_element(By.XPATH, article_selector)\n",
    "            \n",
    "            # Extract the <ul> with class \"actions\" within the article\n",
    "            ul_element = article_element.find_element(By.CSS_SELECTOR, \"ul.actions\")\n",
    "            \n",
    "            # Initialize a dictionary to store data for the current article\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract information from the article's <ul> element\n",
    "            try:\n",
    "                article_data[\"Pharmacy Name\"] = ul_element.find_element(By.XPATH, \"./li/h3\").text\n",
    "            except:\n",
    "                article_data[\"Pharmacy Name\"] = \"Not Available\"\n",
    "\n",
    "            for li in ul_element.find_elements(By.XPATH, \"./li\"):\n",
    "                text = li.text.strip()\n",
    "\n",
    "                # Extract specific information based on the li content\n",
    "                if \"License Number:\" in text:\n",
    "                    article_data[\"License Number\"] = li.find_element(By.XPATH, \".//a/span\").text\n",
    "                elif \"License Type:\" in text:\n",
    "                    article_data[\"License Type\"] = text.split(\": \")[-1]\n",
    "                elif \"License Status:\" in text:\n",
    "                    article_data[\"License Status\"] = text.split(\": \")[-1]\n",
    "                elif \"Expiration Date:\" in text:\n",
    "                    article_data[\"Expiration Date\"] = text.split(\": \")[-1]\n",
    "                elif \"Secondary Status:\" in text:\n",
    "                    article_data[\"Secondary Status\"] = text.split(\": \")[-1]\n",
    "                elif \"City:\" in text:\n",
    "                    article_data[\"City\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "                elif \"State:\" in text:\n",
    "                    article_data[\"State\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "                elif \"County:\" in text:\n",
    "                    article_data[\"County\"] = text.split(\": \")[-1]\n",
    "                elif \"Zip:\" in text:\n",
    "                    article_data[\"Zip\"] = text.split(\": \")[-1]\n",
    "\n",
    "            # Append the article data to the list, avoiding duplicates\n",
    "            if article_data not in articles:\n",
    "                articles.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article with ID {article_id}: {e}\")\n",
    "            continue  # Skip to the next article if an error occurs\n",
    "\n",
    "    # At the end of the loop, `articles` will contain all the scraped data.\n",
    "    print(f\"Scraped {len(articles)} articles.\")\n",
    "\n",
    "    # Step 6: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt7.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "2b02959303a4c9a6",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 8\n",
    "Attempt 7 was a regression; go back and try again "
   ],
   "id": "9f108d4f69b86e49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:06:44.735115Z",
     "start_time": "2024-11-25T03:00:13.090227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Wait for the results to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"post.yes\"))\n",
    "    )\n",
    "\n",
    "    # Step 5: Scroll and Load More Results\n",
    "    # Scroll through the page to load all articles before scraping\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "\n",
    "    while True:\n",
    "        # Find all article elements that match the criteria (using CSS class 'post.yes')\n",
    "        new_articles = driver.find_elements(By.CSS_SELECTOR, \"article.post.yes\")\n",
    "        articles.extend(new_articles)\n",
    "\n",
    "        # Scroll down the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for the new content to load\n",
    "\n",
    "        # Check if the page height has changed after scrolling. If no new height, stop scrolling.\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 6: Extract Information from Each Article\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Get the ul with class \"actions\" to extract data from the article\n",
    "            ul_element = article.find_element(By.CSS_SELECTOR, \"ul.actions\")\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract Pharmacy Name\n",
    "            try:\n",
    "                article_data[\"Pharmacy Name\"] = ul_element.find_element(By.XPATH, \"./li/h3\").text\n",
    "            except Exception as e:\n",
    "                article_data[\"Pharmacy Name\"] = \"Not Available\"\n",
    "\n",
    "            # Loop through each <li> in the <ul> and extract relevant data\n",
    "            for li in ul_element.find_elements(By.XPATH, \"./li\"):\n",
    "                text = li.text.strip()\n",
    "\n",
    "                # Extract specific data based on the li's content\n",
    "                if \"License Number:\" in text:\n",
    "                    article_data[\"License Number\"] = li.find_element(By.XPATH, \".//a/span\").text\n",
    "                elif \"License Type:\" in text:\n",
    "                    article_data[\"License Type\"] = text.split(\": \")[-1]\n",
    "                elif \"License Status:\" in text:\n",
    "                    article_data[\"License Status\"] = text.split(\": \")[-1]\n",
    "                elif \"Expiration Date:\" in text:\n",
    "                    article_data[\"Expiration Date\"] = text.split(\": \")[-1]\n",
    "                elif \"Secondary Status:\" in text:\n",
    "                    article_data[\"Secondary Status\"] = text.split(\": \")[-1]\n",
    "                elif \"City:\" in text:\n",
    "                    article_data[\"City\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "                elif \"State:\" in text:\n",
    "                    article_data[\"State\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "                elif \"County:\" in text:\n",
    "                    article_data[\"County\"] = text.split(\": \")[-1]\n",
    "                elif \"Zip:\" in text:\n",
    "                    article_data[\"Zip\"] = text.split(\": \")[-1]\n",
    "\n",
    "            # Append article data to results list\n",
    "            if article_data not in results:\n",
    "                results.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "            continue  # Skip to the next article if there's an error\n",
    "\n",
    "    # At the end of the loop, `results` will contain all the scraped data\n",
    "    print(f\"Scraped {len(results)} articles.\")\n",
    "\n",
    "    # Step 7: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt8.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "cfef7c5c216b5133",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 9:\n",
    "Explicitly try loading all values first, and then return and loop through them.  "
   ],
   "id": "2bd25beb38bbc449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:12:40.295969Z",
     "start_time": "2024-11-25T03:07:16.998649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Scroll to the bottom of the page to load all articles\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll to load all articles\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for the new content to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 5: Loop through all articles by ID (from 0 to 844)\n",
    "    results = []\n",
    "    for article_id in range(845):  # ID range from 0 to 844\n",
    "        try:\n",
    "            # Construct the article selector dynamically using XPath\n",
    "            article_selector = f\"//article[@id='{article_id}']\"\n",
    "            \n",
    "            # Locate the article element by XPath\n",
    "            article_element = driver.find_element(By.XPATH, article_selector)\n",
    "            \n",
    "            # Locate the <ul> element with class \"actions\" within the article\n",
    "            ul_element = article_element.find_element(By.CSS_SELECTOR, \"ul.actions\")\n",
    "            \n",
    "            # Initialize a dictionary to store data\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract the pharmacy name (h3 within the <ul>)\n",
    "            try:\n",
    "                article_data[\"Pharmacy Name\"] = ul_element.find_element(By.XPATH, \"./li/h3\").text\n",
    "            except:\n",
    "                article_data[\"Pharmacy Name\"] = \"Not Available\"\n",
    "\n",
    "            # Extract data for each relevant field\n",
    "            for li in ul_element.find_elements(By.XPATH, \"./li\"):\n",
    "                text = li.text.strip()\n",
    "\n",
    "                if \"License Number:\" in text:\n",
    "                    article_data[\"License Number\"] = li.find_element(By.XPATH, \".//a/span\").text\n",
    "\n",
    "                elif \"License Type:\" in text:\n",
    "                    article_data[\"License Type\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"License Status:\" in text:\n",
    "                    article_data[\"License Status\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"Expiration Date:\" in text:\n",
    "                    article_data[\"Expiration Date\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"Secondary Status:\" in text:\n",
    "                    article_data[\"Secondary Status\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"City:\" in text:\n",
    "                    article_data[\"City\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "\n",
    "                elif \"State:\" in text:\n",
    "                    article_data[\"State\"] = li.find_element(By.XPATH, \".//span\").text\n",
    "\n",
    "                elif \"County:\" in text:\n",
    "                    article_data[\"County\"] = text.split(\": \")[-1]\n",
    "\n",
    "                elif \"Zip:\" in text:\n",
    "                    article_data[\"Zip\"] = text.split(\": \")[-1]\n",
    "\n",
    "            # Avoid duplicates in the results\n",
    "            if article_data not in results:\n",
    "                results.append(article_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article with ID {article_id}: {e}\")\n",
    "\n",
    "    # At the end of the loop, `results` will contain all the scraped data\n",
    "    print(f\"Scraped {len(results)} articles.\")\n",
    "\n",
    "    # Step 6: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt9.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "38eacf936a2e466",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "2b586be1a50ceac1",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 10 \n",
    "Last attempt generated almost all of the pharmacies, but title only.  \n",
    "Continue trying to pick up all of the items on the page.  "
   ],
   "id": "b4c1d8e48e0178d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:24:39.263206Z",
     "start_time": "2024-11-25T03:18:04.852559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Implement infinite scrolling\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "\n",
    "    while True:\n",
    "        # Scroll to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for new content to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get new height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "        # Extract data from all articles on the page\n",
    "        article_elements = driver.find_elements(By.CSS_SELECTOR, \".post.yes\")\n",
    "        \n",
    "        for element in article_elements:\n",
    "            try:\n",
    "                article_data = {}\n",
    "\n",
    "                # Extract pharmacy name (h3 within the <ul>)\n",
    "                h3_element = element.find_element(By.XPATH, \".//li/h3\")\n",
    "                article_data[\"Pharmacy Name\"] = h3_element.text.strip()\n",
    "\n",
    "                # Extract other data fields\n",
    "                for li in element.find_elements(By.CSS_SELECTOR, \".actions li\"):\n",
    "                    text = li.text.strip()\n",
    "                    key_value = text.split(\": \", 1)\n",
    "                    if len(key_value) == 2:\n",
    "                        key, value = key_value\n",
    "                        article_data[key] = value.strip()\n",
    "\n",
    "                articles.append(article_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "    # Step 5: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt10.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "98c5f30d4a78a989",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "80649e8f6a9c0405",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 11\n",
    "Last one got all the items but looped repeatedly through the first 20 or so results, we still need to capture the totality of the results on the page. "
   ],
   "id": "9ad7166443753c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:34:02.145450Z",
     "start_time": "2024-11-25T03:27:29.676116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Implement infinite scrolling\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles = []\n",
    "\n",
    "    while True:\n",
    "        # Scroll to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for new content to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get new height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "        # Extract data from all articles on the page\n",
    "        article_elements = driver.find_elements(By.CSS_SELECTOR, \".post.yes\")\n",
    "        \n",
    "        for element in article_elements:\n",
    "            try:\n",
    "                article_data = {}\n",
    "\n",
    "                # Extract pharmacy name (h3 within the <ul>)\n",
    "                h3_element = element.find_element(By.XPATH, \".//li/h3\")\n",
    "                article_data[\"Pharmacy Name\"] = h3_element.text.strip()\n",
    "\n",
    "                # Extract other data fields\n",
    "                for li in element.find_elements(By.CSS_SELECTOR, \".actions li\"):\n",
    "                    text = li.text.strip()\n",
    "                    key_value = text.split(\": \", 1)\n",
    "                    if len(key_value) == 2:\n",
    "                        key, value = key_value\n",
    "                        article_data[key] = value.strip()\n",
    "\n",
    "                articles.append(article_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "    # Step 5: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt11.csv\", index=False)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "c171cd5e18210f3b",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "21e31b07cce006ad",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 12\n",
    "Still having issues with the data repeating, we will try again to grab beyond the first 20 results.  "
   ],
   "id": "a2cf763cec74760c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:39:22.730540Z",
     "start_time": "2024-11-25T03:38:59.751083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Extract data from all ul elements with class \"actions\"\n",
    "    actions_elements = driver.find_elements(By.CSS_SELECTOR, \".post.yes ul.actions\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for element in actions_elements:\n",
    "        try:\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract pharmacy name (h3 within the <ul>)\n",
    "            h3_element = element.find_element(By.XPATH, \".//li/h3\")\n",
    "            article_data[\"Pharmacy Name\"] = h3_element.text.strip()\n",
    "\n",
    "            # Extract other data fields\n",
    "            for li in element.find_elements(By.CSS_SELECTOR, \".actions li\"):\n",
    "                text = li.text.strip()\n",
    "                key_value = text.split(\": \", 1)\n",
    "                if len(key_value) == 2:\n",
    "                    key, value = key_value\n",
    "                    article_data[key] = value.strip()\n",
    "\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "\n",
    "    # Step 5: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt12.csv\", index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "9c7628366b3d1bd",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 13\n",
    "Still trying - only got first 20 records again "
   ],
   "id": "9df9fa3d3892b21d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:55:53.219402Z",
     "start_time": "2024-11-25T03:44:19.544396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Implement a more robust infinite scrolling mechanism\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "        # Extract data after each scroll\n",
    "        actions_elements = driver.find_elements(By.CSS_SELECTOR, \".post.yes ul.actions\")\n",
    "        \n",
    "        for element in actions_elements:\n",
    "            try:\n",
    "                article_data = {}\n",
    "\n",
    "                # Extract pharmacy name (h3 within the <ul>)\n",
    "                h3_element = element.find_elements(By.CSS_SELECTOR, \"li > h3\")\n",
    "                if h3_element:\n",
    "                    article_data[\"Pharmacy Name\"] = h3_element[0].text.strip()\n",
    "\n",
    "                # Extract other data fields\n",
    "                for li in element.find_elements(By.CSS_SELECTOR, \".actions li\"):\n",
    "                    text = li.text.strip()\n",
    "                    key_value = text.split(\": \", 1)\n",
    "                    if len(key_value) == 2:\n",
    "                        key, value = key_value\n",
    "                        article_data[key] = value.strip()\n",
    "\n",
    "                articles.append(article_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "    # Step 5: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies.csv\", index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "701bb6ecdb156f04",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "7fc546c7aa542491",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 14\n",
    "If the above fails I will try this which was combined from the results that got most of the names with the one that got all the fields for the first 20 items "
   ],
   "id": "b2b64ac589da7176"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T04:04:50.521105Z",
     "start_time": "2024-11-25T03:57:21.681370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Implement a more robust infinite scrolling mechanism\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 4: Extract data from all ul elements with class \"actions\"\n",
    "    actions_elements = driver.find_elements(By.CSS_SELECTOR, \".post.yes ul.actions\")\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    for element in actions_elements:\n",
    "        try:\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract pharmacy name (h3 within the <ul>)\n",
    "            h3_element = element.find_elements(By.CSS_SELECTOR, \"li > h3\")\n",
    "            if h3_element:\n",
    "                article_data[\"Pharmacy Name\"] = h3_element[0].text.strip()\n",
    "\n",
    "            # Extract other data fields\n",
    "            for li in element.find_elements(By.CSS_SELECTOR, \".actions li\"):\n",
    "                text = li.text.strip()\n",
    "                key_value = text.split(\": \", 1)\n",
    "                if len(key_value) == 2:\n",
    "                    key, value = key_value\n",
    "                    article_data[key] = value.strip()\n",
    "\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "\n",
    "    # Step 5: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    #Drop Rows that are all NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt14.csv\", index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "1f6ae8778cdeb47f",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "b25e51f990aeb5cd",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 15\n",
    "Previous attempts all continue to stall out after 20 pharmacies; I have yet to be able to capture all of the pharmacies on the page.  "
   ],
   "id": "9ea3aa8e6eb29d5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T04:20:25.862605Z",
     "start_time": "2024-11-25T04:08:53.958085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def extract_data():\n",
    "    \"\"\"Helper function to extract pharmacy data\"\"\"\n",
    "    actions_elements = driver.find_elements(By.CSS_SELECTOR, \".post.yes ul.actions\")\n",
    "    articles = []\n",
    "    \n",
    "    for element in actions_elements:\n",
    "        try:\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract pharmacy name (h3 within the <ul>)\n",
    "            h3_element = element.find_elements(By.CSS_SELECTOR, \"li > h3\")\n",
    "            if h3_element:\n",
    "                article_data[\"Pharmacy Name\"] = h3_element[0].text.strip()\n",
    "\n",
    "            # Extract other data fields\n",
    "            for li in element.find_elements(By.CSS_SELECTOR, \".actions li\"):\n",
    "                text = li.text.strip()\n",
    "                key_value = text.split(\": \", 1)\n",
    "                if len(key_value) == 2:\n",
    "                    key, value = key_value\n",
    "                    article_data[key] = value.strip()\n",
    "\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Implement scrolling to grab records in chunks of 20\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Container for all the data\n",
    "    all_articles = []\n",
    "    \n",
    "    while True:\n",
    "        # Extract first 20 records (or fewer if there are no more records)\n",
    "        articles = extract_data()\n",
    "        all_articles.extend(articles)\n",
    "        \n",
    "        # Scroll down to bottom to load more data\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # No new content loaded, break the loop\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Step 4: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(all_articles)\n",
    "\n",
    "    # Drop rows that are all NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt15.csv\", index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "b0fafeb2dc6223ba",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "1a9a694260e1f12c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 16:  Combine Selenium & Beautiful Soup\n",
    "If the above does not work I will try to use Selenium to return the source HTML and then use beautifulsoup to parse the html properly.  "
   ],
   "id": "cdf87ce6211a632"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T04:26:00.283973Z",
     "start_time": "2024-11-25T04:21:16.122820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def scroll_to_bottom():\n",
    "    \"\"\"Scroll the entire page to load all content\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    SCROLL_PAUSE_TIME = 3  # Adjust this based on how long the page takes to load\n",
    "    \n",
    "    while True:\n",
    "        # Scroll to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        # Calculate the new scroll height and compare it to the previous height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Exit loop when no more content is loaded\n",
    "        last_height = new_height\n",
    "\n",
    "def extract_data_from_html(html):\n",
    "    \"\"\"Extract data from the given HTML using BeautifulSoup\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    # Extract pharmacy data from the soup object\n",
    "    actions_elements = soup.select(\".post.yes ul.actions\")\n",
    "    \n",
    "    for element in actions_elements:\n",
    "        try:\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract pharmacy name (h3 within the <ul>)\n",
    "            h3_element = element.select(\"li > h3\")\n",
    "            if h3_element:\n",
    "                article_data[\"Pharmacy Name\"] = h3_element[0].text.strip()\n",
    "\n",
    "            # Extract other data fields\n",
    "            for li in element.select(\".actions li\"):\n",
    "                text = li.text.strip()\n",
    "                key_value = text.split(\": \", 1)\n",
    "                if len(key_value) == 2:\n",
    "                    key, value = key_value\n",
    "                    article_data[key] = value.strip()\n",
    "\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Scroll the entire page to load all content\n",
    "    scroll_to_bottom()\n",
    "\n",
    "    # Step 5: Get the entire page source after scrolling\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Step 6: Extract data from the HTML source using BeautifulSoup\n",
    "    articles = extract_data_from_html(html)\n",
    "\n",
    "    # Step 7: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Drop rows that are all NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt_with_soup.csv\", index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "7ad5e3cc5e68b408",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "39026a6e2ec20945",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 17: Soup Part 2\n",
    "Last attempt with soup did return a table with ... 20 responses.  Still need to figure out how we can get all of them! "
   ],
   "id": "7488aec83bbb3d19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T04:32:27.255838Z",
     "start_time": "2024-11-25T04:31:53.670070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def scroll_to_bottom_until_end_text():\n",
    "    \"\"\"Scroll the entire page to load all content until the end text is visible.\"\"\"\n",
    "    SCROLL_PAUSE_TIME = 3  # Adjust this based on how long the page takes to load\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # Scroll to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        # Check if the \"You have reached the end of your results\" text is present\n",
    "        try:\n",
    "            end_text = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//*[contains(text(), 'You have reached the end of your results')]\"))\n",
    "            )\n",
    "            if end_text:\n",
    "                break\n",
    "        except:\n",
    "            # If the end text isn't found, we continue scrolling\n",
    "            pass\n",
    "        \n",
    "        # Calculate the new scroll height and compare it to the previous height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Exit loop when no more content is loaded\n",
    "        last_height = new_height\n",
    "\n",
    "def extract_data_from_html(html):\n",
    "    \"\"\"Extract data from the given HTML using BeautifulSoup.\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    # Extract pharmacy data from the soup object\n",
    "    actions_elements = soup.select(\".post.yes ul.actions\")\n",
    "    \n",
    "    for element in actions_elements:\n",
    "        try:\n",
    "            article_data = {}\n",
    "\n",
    "            # Extract pharmacy name (h3 within the <ul>)\n",
    "            h3_element = element.select(\"li > h3\")\n",
    "            if h3_element:\n",
    "                article_data[\"Pharmacy Name\"] = h3_element[0].text.strip()\n",
    "\n",
    "            # Extract other data fields\n",
    "            for li in element.select(\".actions li\"):\n",
    "                text = li.text.strip()\n",
    "                key_value = text.split(\": \", 1)\n",
    "                if len(key_value) == 2:\n",
    "                    key, value = key_value\n",
    "                    article_data[key] = value.strip()\n",
    "\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "try:\n",
    "    # Step 1: Open the webpage\n",
    "    url = \"https://search.dca.ca.gov/advanced\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait for the filters to load and set them (Primary Status and License Type)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"primaryStatusCodes\"))\n",
    "    )\n",
    "\n",
    "    # Set the Primary Status to \"Active\"\n",
    "    status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "    status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "    # Set the License Type to \"Sterile Compounding Pharmacy\"\n",
    "    license_dropdown = Select(driver.find_element(By.ID, \"licenseType\"))\n",
    "    license_dropdown.select_by_visible_text(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "    # Step 3: Wait for the \"Search\" button and click it\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"srchSubmitHome\"))\n",
    "    )\n",
    "    search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Scroll the entire page to load all content until we reach the \"end of results\" text\n",
    "    scroll_to_bottom_until_end_text()\n",
    "\n",
    "    # Step 5: Get the entire page source after scrolling\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Step 6: Extract data from the HTML source using BeautifulSoup\n",
    "    articles = extract_data_from_html(html)\n",
    "\n",
    "    # Step 7: Save the results to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Drop rows that are all NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Print or save the DataFrame\n",
    "    print(df)\n",
    "    df.to_csv(\"pharmacies_attempt_with_end_text.csv\", index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ],
   "id": "b91f535fbc0c5538",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T04:30:37.424300Z",
     "start_time": "2024-11-25T04:30:37.414956Z"
    }
   },
   "cell_type": "code",
   "source": "html",
   "id": "d7ea5bcb158d96b",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7d1048049ffd01aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attempt 18 with selenium and soup ",
   "id": "6f1d5ebe527f57b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T04:41:05.905922Z",
     "start_time": "2024-11-25T04:40:32.522123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# 1. Navigate to the URL\n",
    "driver.get(\"https://search.dca.ca.gov/advanced\")\n",
    "\n",
    "# 2. Wait until you see the ID \"srchSubmitHome\"\n",
    "WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"srchSubmitHome\")))\n",
    "\n",
    "# 3. Select \"Active\" from the dropdown by ID \"primaryStatusCodes\"\n",
    "status_dropdown = driver.find_element(By.ID, \"primaryStatusCodes\")\n",
    "status_dropdown.click()\n",
    "status_dropdown.find_element(By.XPATH, \"//option[text()='Active']\").click()\n",
    "\n",
    "# 4. Enter \"Sterile Compounding Pharmacy\" in the licenseType field\n",
    "license_type_field = driver.find_element(By.ID, \"licenseType\")\n",
    "license_type_field.send_keys(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "# 5. Click the search button by ID \"srchSubmitHome\"\n",
    "search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "search_button.click()\n",
    "\n",
    "# 6. Allow the page to load and scroll until \"You have reached the end of your results\"\n",
    "time.sleep(5)  # Let the page load\n",
    "\n",
    "# Scroll to the bottom until the \"You have reached the end of your results\" text is found\n",
    "while True:\n",
    "    page_end_element = driver.find_elements(By.XPATH, \"//*[text()='You have reached the end of your results']\")\n",
    "    if page_end_element:\n",
    "        break\n",
    "    # Scroll down\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# 7. Get the entire HTML of the page\n",
    "html = driver.page_source\n",
    "\n",
    "# 8. Parse the HTML with BeautifulSoup and extract the data\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "articles = []\n",
    "\n",
    "# Find all article tags with the ul tag having class \"actions\"\n",
    "for article in soup.find_all(\"article\"):\n",
    "    actions_element = article.find(\"ul\", class_=\"actions\")\n",
    "    if actions_element:\n",
    "        article_data = {}\n",
    "\n",
    "        # Extract pharmacy name (h3 within the <ul>)\n",
    "        h3_element = actions_element.find(\"h3\")\n",
    "        if h3_element:\n",
    "            article_data[\"Pharmacy Name\"] = h3_element.text.strip()\n",
    "\n",
    "        # Extract other data fields\n",
    "        for li in actions_element.find_all(\"li\"):\n",
    "            text = li.text.strip()\n",
    "            key_value = text.split(\": \", 1)\n",
    "            if len(key_value) == 2:\n",
    "                key, value = key_value\n",
    "                article_data[key] = value.strip()\n",
    "\n",
    "        articles.append(article_data)\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"attempt_18.csv\", index=False)\n",
    "\n",
    "# Optionally print a message confirming that the CSV was saved\n",
    "print(\"CSV file 'attempt_18.csv' has been saved.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ],
   "id": "9ae39acf9f2dc06a",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "dfe937e650ec4e5d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt 19\n",
    "THe previous works minus the filter for active, this attempts to fix that.  "
   ],
   "id": "ef38f0a6a3e18d4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T04:43:39.788541Z",
     "start_time": "2024-11-25T04:43:08.053750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.support.ui import Select  # Import Select for dropdowns\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# 1. Navigate to the URL\n",
    "driver.get(\"https://search.dca.ca.gov/advanced\")\n",
    "\n",
    "# 2. Wait until you see the ID \"srchSubmitHome\"\n",
    "WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"srchSubmitHome\")))\n",
    "\n",
    "# 3. Set the Primary Status to \"Active\"\n",
    "status_dropdown = Select(driver.find_element(By.ID, \"primaryStatusCodes\"))\n",
    "status_dropdown.select_by_visible_text(\"Active\")\n",
    "\n",
    "# 4. Enter \"Sterile Compounding Pharmacy\" in the licenseType field\n",
    "license_type_field = driver.find_element(By.ID, \"licenseType\")\n",
    "license_type_field.send_keys(\"Sterile Compounding Pharmacy\")\n",
    "\n",
    "# 5. Click the search button by ID \"srchSubmitHome\"\n",
    "search_button = driver.find_element(By.ID, \"srchSubmitHome\")\n",
    "search_button.click()\n",
    "\n",
    "# 6. Allow the page to load and scroll until \"You have reached the end of your results\"\n",
    "time.sleep(5)  # Let the page load\n",
    "\n",
    "# Scroll to the bottom until the \"You have reached the end of your results\" text is found\n",
    "while True:\n",
    "    page_end_element = driver.find_elements(By.XPATH, \"//*[text()='You have reached the end of your results']\")\n",
    "    if page_end_element:\n",
    "        break\n",
    "    # Scroll down\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# 7. Get the entire HTML of the page\n",
    "html = driver.page_source\n",
    "\n",
    "# 8. Parse the HTML with BeautifulSoup and extract the data\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "articles = []\n",
    "\n",
    "# Find all article tags with the ul tag having class \"actions\"\n",
    "for article in soup.find_all(\"article\"):\n",
    "    actions_element = article.find(\"ul\", class_=\"actions\")\n",
    "    if actions_element:\n",
    "        article_data = {}\n",
    "\n",
    "        # Extract pharmacy name (h3 within the <ul>)\n",
    "        h3_element = actions_element.find(\"h3\")\n",
    "        if h3_element:\n",
    "            article_data[\"Pharmacy Name\"] = h3_element.text.strip()\n",
    "\n",
    "        # Extract other data fields\n",
    "        for li in actions_element.find_all(\"li\"):\n",
    "            text = li.text.strip()\n",
    "            key_value = text.split(\": \", 1)\n",
    "            if len(key_value) == 2:\n",
    "                key, value = key_value\n",
    "                article_data[key] = value.strip()\n",
    "\n",
    "        articles.append(article_data)\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"attempt_19.csv\", index=False)\n",
    "\n",
    "# Optionally print a message confirming that the CSV was saved\n",
    "print(\"CSV file 'attempt_19.csv' has been saved.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ],
   "id": "9ad30f3d77a5a6de",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "46b3febcd23ac645",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
